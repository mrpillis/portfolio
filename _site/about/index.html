<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-E00L9PT3V9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-E00L9PT3V9');
  </script>
  <title></title>

  <meta content="text/html; charset=utf-8" http-equiv="Content-Type">

  <meta name="author" content="" />
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="/style.css" />
  <link rel="canonical" href="http://localhost:4000/about/">
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <h1>
                Daniel Pillis
              </h1>
              <p>I am a graduate student at the <a href="https://www.media.mit.edu/">Massachusetts Institute of Technology</a>, part of the <a href="https://tangible.media.mit.edu/">Tangible Media Group</a> at <a href="https://web.mit.edu/">the MIT Media Lab</a>, where I work on human computer interaction. My  advisor is <a href="http://www.cs.cmu.edu/~hebert/">Dr. Hiroshi Ishii</a>.
              </p>
              <p>
                From 2018 to 2020, I worked at <a href="https://www.intel.com">Virginia Tech</a>, as part of <a href="https://www.intelrealsense.com/">Institute for Creativity, Arts & Technology</a>. I primarily designed interactive large scale immersive environments, using a variety of sensors, including the Intel RealSense <a href="https://software.intel.com/en-us/articles/realsense-r200-camera">R200</a> and <a href="https://software.intel.com/en-us/realsense/d400">D400</a> motion captures systems. Additionally, I worked on <a href="https://github.com/IntelRealSense/librealsense">room scale pose estimation experiments</a>, <a href="https://patents.google.com/patent/US9992474B2">plant tracking systems</a>, <a href="https://patents.google.com/patent/US9304597">human-computer interaction devices</a>, and helped develop demos for conferences, including SIGGRAPH 2018-20.
              </p>
              <p>
                I have an MFA in Media Art from <a href="http://cs.stanford.edu">Carnegie Mellon University</a>, where I was a research assistant for <a href="https://www.cs.cmu.edu/~cga/">Dr. Chris Atkeson</a> and a teaching assistant for <a href="https://www.cs.cmu.edu/~jkh/">Dr. Jessica Hodgins</a> (<a href="http://vision.stanford.edu/teaching/cs131_fall1617/">15-465</a> & <a href="http://cs231n.stanford.edu/2017/">15-894</a>). I have a BA in Psychology, English & Art from <a href="https://www.rutgers.edu/">Rutgers University</a>, where I worked in <a href="https://www.cmu.edu/dietrich/sds/people/faculty/gretchen-chapman.html">Gretchen Chapman's</a> Decision Making lab.
              </p>
              <p style="text-align:center">
                <a target="_blank" href="https://mailhide.io/e/lCqh3Bhy"> Email</a> &nbsp;/&nbsp;
                <a href="https://github.com/mrpillis/">GitHub</a> &nbsp;/&nbsp;
                <a href="https://www.linkedin.com/in/pillis/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:200%" alt="profile photo" src="images/DPillis_Pic.jpg">
            </td>
          </tr>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:2.5%;width:100%;vertical-align:middle">
              <h2>Research</h2>
              <p>
                I'm interested in simulations, digital twins, artificial intelligence, time, computer graphics and robotics.
              </p>
            </td>
          </tr>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

          
          
          
          
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/1.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 10: 3D Scanner Prototype</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>The final project of this independent study is culminating in a custom 3D scanner. This will be finished on at the end of the semester. I soent about a week troubleshooting the XIAO ESP32 Sense before I finally undersood how to make them work. They need to be booted each time they are loaded, and after the script is run, they then need to be reset. The buttons are so small it is hard to see what to do. Once I figured out this workflow, it was much easier. I can now get a webserver running and had all 6 of the cameras working. The next issue is getting them to work at once. Quentin advised me to adjust the code so that the header is changed which will allow me to control the input of the camera.</p>
<blockquote>
  <p>
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/2.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 9. 3D Scanner Types</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>I experimented with three different approaches to scanning this week. Using the Artec Leo, the 3D sense, and the iPhone Lidar. Each of them produces a different quality scan. Some of these scans can be seen below. <img src="tn/images/tmg-scans.png" alt="drawing" width="200" /> <img src="tn/images/room-combusted.png" alt="drawing" width="200" /> <img src="tn/images/tmg-scans.png" alt="drawing" width="200" /></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/3.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 8: 3D Printer Prototypes</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>This week I learned about different types of 3D printer prototypes and how to make machines, since students in HTMAA had ‘machine week’.</p>
<blockquote>
  <p>
    This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor&#8217;s samples, the system generates constraints that limit motion orthogonal to the rigid body model&#8217;s surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions.
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/4.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 7: 3D Printer Types</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>This week I learned about different types of 3D printers. I worked with Wedyan and Yun to learn how to use the Form Labs printers that we have in the lab and in CBA. Using the I made some new 3D prints of a mechanical robot arm and some sample prints of miniatures from the Tangible Media group lab. My next idea is to use the Stratsys printer to prototype miniatures from the tangible media lab that can actuate. I hope to make a miniature version of Transform and InTouch as part of my final project. The draft version is seen below: <img src="tn/images/transform.jpg" alt="drawing" width="200" /> <img src="tn/images/intouch.JPG" alt="drawing" width="200" /> <img src="tn/images/print4" alt="drawing" width="200" /></p>
<blockquote>
  <p>
    I talked with Ozgun and she suggested we use a rack and pin rotation mechanism inside the print so that we could indiviually move the top pins along some general motion. This seems like an accessible way to make a toy transform that would demonstrate its appeal and fundamental function.
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/5.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 6: Sensors </h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>Our goal is to figure out how to get RGB RAW images from the SEEED STUDIO XIAO ESP32S3 SENSE. <img src="tn/images/camera.jpg" alt="drawing" width="200" /></p>
<blockquote>
  <p>
 <img src="tn/images/input1.png" alt="drawing" width="200" />  <img src="tn/images/input2.png" alt="drawing" width="200" /> <img src="tn/images/input3.png" alt="drawing" width="200" /> 
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/7.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 5. 3D Hardware</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>In early November, I ordered 6 XIAO ESP32S3 cameras. I met with Quentin in CBA to discuss his previous projects exploring custom 3D scanners. His made use of an arm that moved and rotated around an object. We decided that imitating a light stage design would be better, so we devised a plan to use small embedded camera sensors to capture multiple images. We decided to order the following: https://www.seeedstudio.com/OV5640-Camera-for-XIAO-ESP32S3-Sense-With-Heat-Sink-p-5739.html. I started to explore how this might work by implementing the gaussian splatting algorithim locally on my machine so that I could run simple 3D scan processes. Based on these experiements, we came up with a plan to construct a cage or structure rot of in the design of a buckminster fuller ball to capture objects. This specficially applies to my thesis because I need to scan a massive amount of small objects. I describe this plan in an excerpt of my thesis proposal below:
<img src="tn/images/cage-test.png" alt="drawing" width="200" /></p>
<blockquote>
  <p>
In an independent study with Professor Neil Gershenfeld, I am learning how to design custom 3D scanning interfaces to use for personal object archiving. This requires multi-camera capture to sequence multiple images together, resulting in low poly but high resolution 3D models (gaussian splats) of objects. These hardware tools will provide a tangible toolkit for the process of life-logging large archives of personal possessions for use in the simulation environment. In the independent study, I am also developing pipelines to which draw inspiration from Meta’s Codec Avatar pipeline for virtually indistinguishable digital doubles. I am specifically applying this to the relationship between an individual and their environment. This process is connected to the processes explored in the 3D scanning workflows, but evolved to include a proposal for a computational model. The goal is to implement this in the phone booth.
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/6.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 4: 3D Software</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>On October 31st I manufactured six holokits on the laser cutter to use for a tutorial on mixed reality. We wanted the students to be able to make their own holographic displays. <img src="tn/images/holokit.jpg" alt="drawing" width="200" /></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/8.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 3. Contemporary Techniques</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>On October 25th I met with Quentin and we looked at workflows for planning how to make a 3D scanner. We thought of this as a portable lightstage or portable panoptic studio for small objects. 20-30 Rasperry Pi Cameras. Initial plans: Rasperry Pi Zero with Camera Cable-3D Printed Geodesc Dome of Cameras-Expandable Bucky Ball- Compute with SIFT for each image. Stream the data super fast.Goal- Instant 3D object scanning- Top and bottom. Explore contemporary formats like ThreeJS and MeshGPT. I experimented with Polycam’s floorplan tool and it is disappointing. It renders objects as if they are in a domestic setting automatically. It didn’t work for me when I tried to scan an unconventional space. I used grabCad to find models of seeed studio xiao nrf52840 3d model.</p>
<blockquote>
  <p>
  
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/9.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 2: 3D Scanning Workflows</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>On October 24th I met with John to go over the 3D scanning and 3D printing workflows. <img src="tn/images/5.jpg" alt="drawing" width="200" /> 
We compared: Photogrammetry vs Neural Radiance Fields Vs 3D gaussian splats. <img src="tn/images/artex.jpg" alt="drawing" width="200" /></p>

            </td>
          </tr>
          
          
          
          
          
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="/tn/images/10.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Week 1-Principles and practices</h3>
              <br>
              

              
              
              
              
              
              
              
              <p></p>
              <p>On October 19th, for week 1 of the independent study, I met with Dr. Gershenfeld to discuss plans for using the 3D scanners in the E14 workspace of the Center for Bits and Atoms.</p>
<blockquote>
  <p>
  We proposed that I would make holokit prototypes as part of the initial research project into imaging technology. I started by learning how to use the Bambuu 3D printer. Bambuu is state of the art 3D printer that John set me up with in the HTMAA space. I focused my first few weeks on learning now to use the Artec Leo for high quality color 3D scanning. I plan collaborate with shop staff to use the Stratasys J850 to 3D print objects that will be duplicates, inversions, miniatures and variations of the scanned objects.
  </p>
</blockquote>

            </td>
          </tr>
          
          
          
        </table>
        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>

